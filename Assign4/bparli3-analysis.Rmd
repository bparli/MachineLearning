---
title: "CS7641 Assignment4 - MDPs and Reinforcement Learning"
author: "Ben Parli"
date: "April 24, 2016"
output: pdf_document
---

#Introduction
In the final assignemnt we are to explore Markov Decision Processes.  Two MDP problems have been selected and will be analyzed and discussed.  Functions from the R package MDPToolbox are used to implement both the MDP and Reinforcement Learning algorithms.  The rest of the assignment is organized as follows; the selected MDP problems will first be outlined and described, the problems will then be solved and analyzed using Value Iteration and Policy Iteration implemetations from MDPToolbox.  A comparison of these approaches and results will be discussed.  Finally, the problems will be solved using Reinforcement Learning (Q-Learning, again from the MDPToolbox package) with the results and implementation compared to the MDP algorithms.  

#MDP Problems
Two MDP problems have been selected for this assignment; the example Forest problem generated by the MDPToolbox package, and the MDP problem outlined in the Reserves Design paper cited in the references at the end of this assignment.

The example Forest problem is most likely included in the MDPtToolbox package as a means of becoming more familiar with the toolsets and features the package has to offer.  Its described as a "forest is managed by two actions: Wait and Cut. An action is decided each year with first the objective to maintain an old forest for wildlife and second to make money selling cut wood. Each year there is a probability p that a fire burns the forest." The objective, then, is to maximize the reward given a definitive payout (selling the wood) and given an alternative, but higher, payout, but one subject to external risks.  A risk averse individual would likely opt for the sure thing, selling the wood.  Meanwhile a risk thirsty individual would opt for the ultimate payout once all the states have been traversed.  Interesting to note, the more states there are in the problem, the more likely the strategy of waiting results in the burned forest.  While a simple problem, we can use it to illustrate the differences between value iteration and policy iteration.  The differences in iterations, computation time, and resulting answers can be compared as the number of states in the problem are increased.

Additionally, the forest problem shows real-world implications.  How should we approach the consumption of a precious resource given external factors outside of our control, but with a static probability? Many problems would seem to reduce to this decison problem such as utility planning, water demand or any demand planning, organization of workers, even economic policy.

The second MDP problem is taken from a previous paper addressing species preservation.  From the paper's abstract; "The principle focus of the reserve design literature is on determining which sites to reserve to maximise the number of species conserved. To each site is attached species which become conserved when the site is reserved. A good reservation policy is one that conserves as many species as possible." Using the problem generation included with the example, P (transition probability matrix) and R (reward matrix) matrices of 2187 states are created.  This problem will not be iterated over different numbers of states to compare and contrast the algorithms as with the Forest problem, but rather the algorithms will be applied to this specific generated problem.  Here again, however, the problem of how to best utilize finite resources in order to maximize utility (in this case preserving the greatest number of species) is presented in the Species Preservation Problem.  Any sort of philanthropy can relate to the Species Preservation Problem as can processes which reduce to a basic triage of precious resources.  

#MDP Approach
The MDP algorithms are run against each MDP problem and comapred a few ways: by number of iterations, computing time, and by the policy choices each made.  The Policy choice is averaged for each MDP probelm to get a rough estimate of the action an MDP algorithm typically took.  The Forest Consumption problem has only two choices, wait for the forest to grow or cut the forest for wood.  The Species Preservation is more complex though with 6 choices, each representing a site to conserve.

##The Forest Consumption Problem 
Using the example forest problem generation function in the MDPToolbox package problem sizes increasing from 10 to 1000 are created.  The Value Iteration and Policy Iteration algorithms are looped over applied to each and plotted for analysis.  The average choice is also captured for each algorithm.  That is, if choice 1 is made to wait until the end for the final payoff or if 2 is more often made?  The number of iterations and the amount of computing time is also captured.  Additionally, the final policy of each iteration is evaluated in order to compare the policy output by value iteration algorithm and the policy output by policy iteration algorithm. 

We know Policy Iteration converges faster than Value Iteration under some conditions due to its more efficient nature.  Although individual Policy Iterations are more expensive than individual Value Iterations, there are much fewer of them since it is "only" solving a linear equation.  In the plot below this is clearly the case.  As the number of states in the problem grows to 1000, the Policy Iteration also grows at a slower rate than Value Iteration.

```{r fig.width=5, fig.height=5,fig.show="hold", echo=FALSE, message=FALSE}
library(png)
library(grid)

img1 <- readPNG("time.png")
grid.raster(img1)

```

The comparison in number of iterations below also support this expected behavior and displays a much clearer difference from even the initial, low number of states.  The Policy Iterations algorithm maintains a steady number of iterations even as the number of states grow to 1000.  The Value Iterations algorithms quickly grows with the number of states but starts to flatten its slope around 100 states.  It still shows an upward trend at 1000 states.  I*ntuitively, this is expected since with more states it will take more iterations of the Value Iteration algorithm to propogate the Reward of each state ("truth") throughout.

```{r fig.width=5, fig.height=5,fig.show="hold", echo=FALSE, message=FALSE}
library(png)
library(grid)

img1 <- readPNG("iterations.png")
grid.raster(img1)

```

The choices of the two algorithms can also be compared as the number of states are increased.  Since both algorithms are rooted in the Bellman equation it can be expected they will somewhat agree on otimal policy choices even though they are solving the problem differently (Policy Iteration as a linear equation but Value Iteration as a more iterative algorithm).  The graph below shows this to be the case, although the Value Iterations algorithm preferred choice 1 a little more often than the Policy Iteration algorithm.  As the number of states increased, the second choice became a more obvious choice and the difference in choices between the two algorithms decreases.  This convergence in thinking as the number of states grows is likely a function of the problem than the algorithms, however.  With more states, the likelihood of encountering a fire at some point rises.  Therefore, the safe choice (cut the wood) starts to become the more obvious better choice as the number of states in the problem increase.  

```{r fig.width=5, fig.height=5,fig.show="hold", echo=FALSE, message=FALSE}
library(png)
library(grid)

g1 <- readPNG("policy_choice.png")

grid.raster(img1)

```


##The Species Preservation Problem
Value Iteration and Policy Iteration are again applied to the Species Preservation problem already described.  The generated problem is 2187 states and made up of 20 sites and 7 species from which to conserve.  Each site is either Available, Reserved, or Developed at the beginning of the time period.  The development process is assumed to be irreversible.  Similarly, once a site is selected as a reserve, all species occurring at that site are protected and assumed to survive. Sites selected as reserves are assumed to remain protected forever. The third category of land use, "natural", can either remain natural, be developed, or be selected as a reserve.  There is a cost associated with preserving each site.  If a site starts a time period as natural and is not selected as a reserve during that time persiod period, then it is developed at the end of the period with some probability (P) and stays in its natural state with probability (1-P).  

Again, the number of iterations to convergence, computing time, and average choice are compared.  The average choice is an admittedly rough gauge of similarity in that the two algorithms can easily make different choices on the same state, but have a close average overall.  Still, it provides some rough insight into which choice each preferred over the lifetime of the algorithm; in this case 3.  

Interestingly, in this MDP problem, the Policy Iteration took longer computing time, although the number of iterations shows a similar pattern.  A possible hypothesis for this observation in time is the number of choices.  In this MDP problem there are six possible actions at each state, whereas in the Forest Consumption problem there were only two. Since the Policy Iteration algorithm performs a lookahead of each action, this may explain the change in time.  Also, the number of Policy iterations was not very large for this MDP problem relative to the Forest Problem above.  Since each Policy iteration is more expensive than each Value Iteration, the smaller difference in iterations was not enough to make the Value Iteration algorithm more time effiecient in this case.  

###MDP Comparison

|   Model           |  Iterations   |  Average Choice (1-6) | Time (s)    | Value Function Eval.  |
|---                |---            |---                    |---          |---                    |
| Value Iteration   |     5         |     2.69              |   6.8       |     9.4               |
| Policy Iteration  |     2         |     2.55              |   7.9       |     10.5              |


#Reinforcement Learning
The Q Learning algorithm is used for the Reinforcement Learning section of the assignment.  It is again taken from the R package MDPToolbox.  The Q Learning algorithm is compared for its choices as well as its final value function.  Since Q Learning is not afforded the model the MDP algorithms were, it will essentially trade learning time for a priori knowledge.    

This Q Learning algorithm implementation itself is a greedy search algorithm.  The choice of actions is probabilistic with probability 1-(1/log(n+2)) where n is the iteration.  Therefore, similar to what was discussed in lecture, the chosen action is likely to lead to the highest utility of the previously discovered states at higher iterations.  At lower iterations it is more likely to explore actions it has not yet encountered.   Additionally, the learning rate of Q is (1/sqrt(n+2)).  Therefore, the learning rate too will decay as the number of iterations increase.    

The plots below compare the three (Value Iteration, Policy Iteration and Q Learning).  The Q Learning algorithm actually converges to 1 as the states increase whereas the other two algorithms trend towards the second choice.

```{r fig.width=5, fig.height=5,fig.show="hold", echo=FALSE, message=FALSE}
library(png)
library(grid)

img1 <- readPNG("policy_choices.png")

grid.raster(img1)

```

Here again the Q Learning algorithm shows a clear difference from the other two algorithms.  Although computing time is not graphed here, the time taken by the Q Learning algorithm was significantly more than either of the MDP algorithms.  Since Q Learning visits each state and, by theory, requires an infinite number of iterations, it seems clear the default 10,000 iterations is not enough to approach an optimal solution to this problem.

In order to attempt to compare and converge the problems a final, greater experiment is performed on the Forest Consumption problem.  At 100 states the Q Learning algorithm is allowed 100,000,000 iterations.  

We can compare the choices each makes with the rules of the problem in mind.  Only if the choice is to wait in each state until the very end without the forest burning will the payoff be maximal.  However, each algorithm is assigning a certain value to the states in which the choice is made to wait, even though the next state the choice is made to cut the forest.  That is, the states in which the wait action is taken is a wasted iteration since it will never payoff given the subsequent choice to cut.  One possible reason is, these algorithms have an infinite horizon.  They are not aware of an end state and as a result are mis-evaluating some of the optimal actions.  The Value Iteration algorithm is more guilty of this; of the 100 choices it chooses to cut only 74 times.  The Policy Iteration algorithm chooses to cut almost all of the time at 96.  The Q Learning algorithm was almost split at lower iterations (100000), however, the number of cutting choices started to climb with more allowed iterations of Q-Learning though still didn't approach Value Iteration or Policy Iteration.  

Fortunately, the Forest Problem is simple to evaluate as a policy since the choices where waiting is made is an obvious misstep in the decsion algorithm since one must wait the entire time index in order to receive the payout.  To quantify this further the utilities are summed in the value function outputs only where the algorithm did not misstep by choosing to wait and followed by a choice to cut.  This is the total utility represented in the final column only as a means of comparison.

###MDP and RL Comparison - Forest Consumption

|   Model           |  Iterations   |  Average Choice (1-6) | Time (s)    | Total Utility |
|---                |---            |---                    |---          |---            |
| Value Iteration   |     56        |     1.74              |   0.14      |   371         |
| Policy Iteration  |     2         |     1.98              |   0.16      |   492.7       |
| Q Learning        |     100M      |     1.44              |   N/A       |   208.5       |

The Species Preservation proved much more difficult to compare as adequately due to the number of states in the problem (2187) and the additional actions (7 species and 20 sites).  "Only" 1 million iterations of Q Learning were allowed to run with the resulting in the above outcomes and comparisons to the MDP algorithms.   Additionally, since the strategy is less obvious in this problem, the TD algorithm is used to evaluate each resulting value function.  Again the Q Learning problem diverges in choice from the MDP algorithms.  This is still not surprising given the MDP algorithms are so similar to one another (rooted in the Bellman equation and model-based) and so different from the RL Q Learning algorithm (model-free).  In this case, however, a more difficult tradeoff is found.  The Q Learning algorithm has done a better job with this problem if the TD algorithm evaluation is accepted as the performance measure.  However, the time to arrive at this solution was exceptionally long with ~1M more iterations.  In this case the problem expert would need to determine whether wrangling this last little bit of decision performance is worth the additional time and computing resources.  

Of course, a tradeoff implies the Rewards are known.  If the rewards are not known ahead of time, the results of this experiment indicate that with enough computing time, the model-free Q-Learning algorithm will find an optimal solution.   

###MDP and RL Comparison - Species Preservation

|   Model           |  Iterations   |  Average Choice (1-6) | Time (s)    | Value Funct. Eval.  |
|---                |---            |---                    |---          |---                  |
| Value Iteration   |     5         |     2.69              |   6.8       |     9.4             |
| Policy Iteration  |     2         |     2.55              |   7.9       |     10.5            |
| Q Learning        |     1M        |     1.88              |   N/A       |     11.5            |

# Final Takeaways
This assignment highighted MDP problems and how they can be solved with MDP algorithms when the model and Rewards functions are known, and with Reinforcement Learning when the model and Rewards functions are not known.  Findings have been discussed throughout, but some additional takeaways are the following:

* The MDP algorithms arrive at similar choices in policy, but sometimes only eventually.  The differences are more obvious in number of iterations to convergence and computing time as discussed above.
* The MDP algorithms and, to a greater extent, the Q Learning algorithm struggled with the Forest Problem.  Interestingly, this was considered to be the simpler of the two.  Tweaking the model and the algorithms did not yield benefits.  Some of the frustrations are discussed above, but another consideration from these observations could lie with the model itself.  The algorithms were rewarding themselves by choosing to wait, when they should not have seen any immediate rewards.  A very explicit model would seem to be extremely important then.  
* In both cases the MDP algorithms resulted in a clearly different policy than the RL algorithm, Q Learning, but converegd must faster.  Faster convergence time was expected, although with more iterations the Q Learning algorithm started to trend towards the MDP algorithms' policies.     

#References and Citations

R Package MDPtoolbox, https://cran.r-project.org/web/packages/MDPtoolbox/MDPtoolbox.pdf

MDP Reserves Design Example, https://mulcyber.toulouse.inra.fr/frs/?group_id=11&release_id=580#mdptoolbox_examples-reserve-design-problem-title-content

Dynamic reserve site selection under contagion risk of deforestation, http://www.maths.uq.edu.au/MASCOS/MODSIM05/Sabbadin.pdf
